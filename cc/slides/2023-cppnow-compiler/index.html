<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
<title>Modernizing Compiler Design for Carbon&#39;s Toolchain</title>
<meta name="description" content="Chandler&#39;s blog about programming, languages, compilers, performance, cpus, tech, and everything else.">

<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><link rel="stylesheet" href="../../cc/reveal-js/dist/reset.css">
<link rel="stylesheet" href="../../cc/reveal-js/dist/reveal.css">
  <link rel="stylesheet" href="../../cc/css/reveal/custom-theme.min.a384516dc0d6cf603f97be9d7a7cdbe8416ea3c86e50058eba175fec65cb3b7c.css" id="theme"><script
  src="../../cc/js/scripts.min.3abf59a79d74ae0e115f2ca793e3e4d0f2d62d936ba3bef280adb69d14af0154.js"
  integrity="sha256-Or9Zp510rg4RXyynk&#43;Pk0PLWLZNro77ygK22nRSvAVQ="
  crossorigin
></script>
<script>
  function beforeHighlightHook(hljs) {
    hljs.registerLanguage('Carbon', carbonLang);
    hljs.addPlugin({
      'after:highlightElement': ({ el, result }) => {
        var pattern =
          /`(?:(?:<span class="hljs-operator">&lt;<\/span>|&lt;)(?:<span class="hljs-number">)?(\d+)(?:<\/span>)?(?:<span class="hljs-operator">&gt;<\/span>|&gt;))?([^`]*)`/g;
        el.innerHTML = el.innerHTML.replace(
          pattern,
          function (match, index, containedText) {
            if (containedText === '') {
              return '`';
            }
            var class_str = 'fragment highlight-code';
            var index_str =
              index === undefined ? '' : `data-fragment-index="${index}"`;
            return `<span class="${class_str}" ${index_str}>${containedText}</span>`;
          }
        );

        
        el.innerHTML = el.innerHTML.replace(/‚ùå/g, '<span class="hljs-emoji">$&</span>');
      },
    });
  }
</script>

  </head>
  <body>
    
    <div class="reveal">
      <div class="slides">
  

    
<section data-noprocess data-shortcode-slide
      data-background-image="cppnow_splash.png">
  
</section><section>
<div class="r-stretch" style="display: flex; flex-direction: column; justify-content: center">
<h1 class="r-fit-text" id="modernizing-compiler-designbrfor-carbons-toolchain">Modernizing Compiler Design<br/>for Carbon&rsquo;s Toolchain</h1>
</div>
<div class="col-container"><div class="col-4">
<h3 id="chandler-carruth-br-chandlerc1024-br-chandlercgooglegmailcom">Chandler Carruth <br/> @chandlerc1024 <br/> chandlerc@{google,gmail}.com</h3>
</div><div class="col right">
<h3 id="cppnow-2023">CppNow 2023</h3>
</div></div>
<div style="text-align: right">
<p><a href="https://chandlerc.blog/slides/2023-cppnow-compiler">https://chandlerc.blog/slides/2023-cppnow-compiler</a></p>
</div>



<aside class="notes"><ul>
<li>Introduce myself, talk about how exciting it is to be back at CppNow!!!</li>
<li>Especially excited to be back here and share a little bit more detail on
Carbon and our strategy here.</li>
</ul>
</aside>
</section>

  

    <section><h1 id="traditional-compiler-design">Traditional compiler design</h1>



<aside class="notes"><p>Let&rsquo;s start with a quick overview of fairly traditional compiler design that you
might find in a university class &hellip;</p>
</aside>
</section><section>
<div class="diagram-center">
<img alt="The 'Dragon Book' cover"
     src="https://m.media-amazon.com/images/I/51FWXX9KWVL.jpg"
     height="800px">
</div>



<aside class="notes"><p>&hellip; or in my original compilers text book, the so-called &ldquo;dragon book&rdquo;.</p>
</aside>
</section><section>
<h2 id="lexer-text--tokens">Lexer: text ‚Üí tokens</h2>
<ul>
<li>Handles text: characters, encodings, unicode, whitespace, comments, &hellip;</li>
<li>Generally a regular language, and implemented with finite automata</li>
<li>Produces encapsulated tokens:
<ul>
<li>Keywords</li>
<li>Punctuation &amp; operators</li>
<li>Identifiers</li>
<li>Custom lexed literals: numbers, strings</li>
</ul>
</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="parser-tokens--ast-abstract-syntax-tree">Parser: tokens ‚Üí AST (Abstract Syntax Tree)</h2>
<ul>
<li>Handles nesting, structure, and relationships <em>between</em> tokens</li>
<li>Generally a context-free or context-sensitive language</li>
<li>Formalized with grammars and/or forms of <em>pushdown</em> automata
<ul>
<li>Common grammar categories: LL, LL(k), LR, LR(k), LALR, &hellip;</li>
<li>Loads of theory in this space</li>
</ul>
</li>
<li>If not using formal grammar, often implemented with a recursive descent parser</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="semantic-analysis-ast--_correct_-ast">Semantic analysis: AST ‚Üí <em>correct</em> AST</h2>
<ul>
<li>Handles name resolution, type checking, and validating the input program</li>
<li>Simple languages mostly reject invalid ASTs from the parser</li>
<li>More complex languages transform and expand the AST
<ul>
<li>Implicit operations</li>
<li>Instantiation or metaprogramming</li>
</ul>
</li>
<li>Most complex languages (C++) also have feedback to change the parser&hellip; üò°</li>
<li>Should be the last stage to detect errors in the program</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="lowering-ast--ir----machine-code">Lowering: AST ‚Üí IR ‚Üí &hellip; ‚Üí machine code</h2>
<ul>
<li>IR: Intermediate Representation</li>
<li>Handles modeling and optimizing the <em>execution</em> of the program on a machine</li>
<li>Often iterative, potentially with successively lower level IRs</li>
<li>Eventually produces machine code that can run on the target machine</li>
<li>Shouldn&rsquo;t fail on a valid input</li>
<li>This is its own whole system below the first phase (LLVM, etc)</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="historical-influences-on-architecture">Historical influences on architecture</h2>
<ul>
<li>Much of this was formalized around machines &amp; languages from &gt;50 years ago</li>
<li>Semantic analysis and lowering were fairly straightforward
<ul>
<li>Simple language rules (like C, or B)</li>
<li>Minimal optimization in lowering</li>
<li>No IRs, just AST ‚Üí machine code (assembly)</li>
</ul>
</li>
<li>Focus on lexing, parsing, and ASTs</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="imagined-direct-model">Imagined direct model</h2>
<pre><code class="language-plaintext">---------
| Lexer |
---------


</code></pre>
</section><section>
<h2 id="imagined-direct-model-1">Imagined direct model</h2>
<pre><code class="language-plaintext">---------     ----------
| Lexer | --&gt; | Parser |
---------     ----------
        ---^--
        Tokens
</code></pre>
</section><section>
<h2 id="imagined-direct-model-2">Imagined direct model</h2>
<pre><code class="language-plaintext">---------     ----------     -------------
| Lexer | --&gt; | Parser | --&gt; | Semantics |
---------     ----------     -------------
        ---^--        ----^-----
        Tokens        Parse Tree
</code></pre>
</section><section>
<h2 id="imagined-direct-model-3">Imagined direct model</h2>
<pre><code class="language-plaintext">---------     ----------     -------------     ------------
| Lexer | --&gt; | Parser | --&gt; | Semantics | --&gt; | Lowering |
---------     ----------     -------------     ------------
        ---^--        ----^-----           -^-
        Tokens        Parse Tree           AST
</code></pre>



<aside class="notes"></aside>
</section><section>
<h2 id="incremental--lazy-parsing-design">Incremental &amp; lazy parsing design</h2>
<pre><code class="language-plaintext">                      ----------                     -------------     ------------
                      |        | -- One function  -&gt; |           |     |          |
                      | Parser |                     | Semantics |     | Lowering |
                      |        |                     |           |     |          |
                      ----------                     -------------     ------------
</code></pre>
</section><section>
<h2 id="incremental--lazy-parsing-design-1">Incremental &amp; lazy parsing design</h2>
<pre><code class="language-plaintext">                      ----------                     -------------     ------------
                      |        | -- One function  -&gt; |           | --&gt; |          |
                      | Parser |                     | Semantics |     | Lowering |
                      |        |                     |           |     |          |
                      ----------                     -------------     ------------
</code></pre>
</section><section>
<h2 id="incremental--lazy-parsing-design-2">Incremental &amp; lazy parsing design</h2>
<pre><code class="language-plaintext">                      ----------                     -------------     ------------
                      |        | -- One function  -&gt; |           | --&gt; |          |
                      | Parser | -- Next function -&gt; | Semantics | --&gt; | Lowering |
                      |        |                     |           |     |          |
                      ----------                     -------------     ------------
</code></pre>
</section><section>
<h2 id="incremental--lazy-parsing-design-3">Incremental &amp; lazy parsing design</h2>
<pre><code class="language-plaintext">                      ----------                     -------------     ------------
                      |        | -- One function  -&gt; |           | --&gt; |          |
                      | Parser | -- Next function -&gt; | Semantics | --&gt; | Lowering |
                      |        | -- Next function -&gt; |           | --&gt; |          |
                      ----------                     -------------     ------------
</code></pre>
</section><section>
<h2 id="incremental--lazy-parsing-design-4">Incremental &amp; lazy parsing design</h2>
<pre><code class="language-plaintext">---------             ----------                     -------------     ------------
|       | &lt;- Next --- |        | -- One function  -&gt; |           | --&gt; |          |
| Lexer |             | Parser | -- Next function -&gt; | Semantics | --&gt; | Lowering |
|       |             |        | -- Next function -&gt; |           | --&gt; |          |
---------             ----------                     -------------     ------------
</code></pre>
</section><section>
<h2 id="incremental--lazy-parsing-design-5">Incremental &amp; lazy parsing design</h2>
<pre><code class="language-plaintext">---------             ----------                     -------------     ------------
|       | &lt;- Next --- |        | -- One function  -&gt; |           | --&gt; |          |
| Lexer |             | Parser | -- Next function -&gt; | Semantics | --&gt; | Lowering |
|       | -- Token -&gt; |        | -- Next function -&gt; |           | --&gt; |          |
---------             ----------                     -------------     ------------
</code></pre>
</section><section>
<h2 id="designed-around-locality-and-_streaming_-compilation">Designed around &ldquo;locality&rdquo; and <em>streaming</em> compilation</h2>
<ul>
<li>All of this came from an idea of locality <em>in the parsed code</em>
<ul>
<li>One token, one AST node at a time</li>
</ul>
</li>
<li>Enables streaming compilation</li>
<li>Rooted in the needs of machines where the source code was often larger than
the machine memory</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="asts-further-exacerbate-the-cache-impact-due-to-pervasive-pointers-for-edges">ASTs further exacerbate the cache impact due to pervasive pointers for edges</h2>



<aside class="notes"><p>ASTs in the traditional model exacerbate these problems. They tend to be sparse,
pointer-based tree data structures. This is an especially expensive data
structure to operate on.</p>
<p>The common operations are to semantically check each node, which typically
involves looking at the children. And we do this recursively bottom up in many
parsers.</p>
<p>Then we do a depth-first traversal of the tree during lowering.</p>
<p>The result is basically doing multiple traversals of each pointer edge in the
tree, which is about as cache-hostile and poorly locality optimized as it gets.</p>
</aside>
</section><section>
<h2 id="and-real-world-asts-are--massive">And real world ASTs are &hellip; massive</h2>
<div class="col-container">
<div class="col">
<pre><code class="language-cpp">#include &lt;vector&gt;

int test_sum(std::vector&lt;int&gt; data) { 
  int result = 0;
  for (const auto&amp; element : data) {
    result += element;
  }
  return result;
}
</code></pre>
</div>
<div class="col fragment">
<pre style="font-size: 0.4em">
FunctionDecl 0x120d95460 <<stdin>:4:1, line:10:1> line:4:5 test_sum 'int (std::vector<int>)'
|-ParmVarDecl 0x120d95368 <col:14, col:31> col:31 used data 'std::vector<int>':'std::vector<int>' destroyed
`-CompoundStmt 0x120dd5d50 <col:37, line:10:1>
  |-DeclStmt 0x120dcde28 <line:5:3, col:17>
  | `-VarDecl 0x120dcdda0 <col:3, col:16> col:7 used result 'int' cinit
  |   `-IntegerLiteral 0x120dcde08 <col:16> 'int' 0
  |-CXXForRangeStmt 0x120dd5c08 <line:6:3, line:8:3>
  | |-<<<NULL>>>
  | |-DeclStmt 0x120dce1d0 <line:6:30>
  | | `-VarDecl 0x120dcdf98 <col:30> col:30 implicit used __range1 'std::vector<int> &' cinit
  | |   `-DeclRefExpr 0x120dcde40 <col:30> 'std::vector<int>':'std::vector<int>' lvalue ParmVar 0x120d95368 'data' 'std::vector<int>':'std::vector<int>'
  | |-DeclStmt 0x120dd2f90 <col:28>
  | | `-VarDecl 0x120dce268 <col:28> col:28 implicit used __begin1 'iterator':'std::__wrap_iter<int *>' cinit
  | |   `-CXXMemberCallExpr 0x120dce408 <col:28> 'iterator':'std::__wrap_iter<int *>'
  | |     `-MemberExpr 0x120dce3d8 <col:28> '<bound member function type>' .begin 0x120db6c60
  | |       `-DeclRefExpr 0x120dce1e8 <col:28> 'std::vector<int>':'std::vector<int>' lvalue Var 0x120dcdf98 '__range1' 'std::vector<int> &'
  | |-DeclStmt 0x120dd2fa8 <col:28>
  | | `-VarDecl 0x120dce310 <col:28> col:28 implicit used __end1 'iterator':'std::__wrap_iter<int *>' cinit
  | |   `-CXXMemberCallExpr 0x120dd2eb0 <col:28> 'iterator':'std::__wrap_iter<int *>'
  | |     `-MemberExpr 0x120dd2e80 <col:28> '<bound member function type>' .end 0x120db6fd0
  | |       `-DeclRefExpr 0x120dce208 <col:28> 'std::vector<int>':'std::vector<int>' lvalue Var 0x120dcdf98 '__range1' 'std::vector<int> &'
  | |-CXXOperatorCallExpr 0x120dd56c0 <col:28> 'bool' '!=' adl
  | | |-ImplicitCastExpr 0x120dd56a8 <col:28> 'bool (*)(const __wrap_iter<int *> &, const __wrap_iter<int *> &) noexcept' <FunctionToPointerDecay>
  | | | `-DeclRefExpr 0x120dd40f8 <col:28> 'bool (const __wrap_iter<int *> &, const __wrap_iter<int *> &) noexcept' lvalue Function 0x120dd3460 'operator!=' 'bool (const __wrap_iter<int *> &, const __wrap_iter<int *> &) noexcept'
  | | |-ImplicitCastExpr 0x120dd40c8 <col:28> 'const __wrap_iter<int *>':'const std::__wrap_iter<int *>' lvalue <NoOp>
  | | | `-DeclRefExpr 0x120dd2fc0 <col:28> 'iterator':'std::__wrap_iter<int *>' lvalue Var 0x120dce268 '__begin1' 'iterator':'std::__wrap_iter<int *>'
  | | `-ImplicitCastExpr 0x120dd40e0 <col:28> 'const __wrap_iter<int *>':'const std::__wrap_iter<int *>' lvalue <NoOp>
  | |   `-DeclRefExpr 0x120dd2fe0 <col:28> 'iterator':'std::__wrap_iter<int *>' lvalue Var 0x120dce310 '__end1' 'iterator':'std::__wrap_iter<int *>'
  | |-CXXOperatorCallExpr 0x120dd5860 <col:28> '__wrap_iter<int *>':'std::__wrap_iter<int *>' lvalue '++'
  | | |-ImplicitCastExpr 0x120dd5848 <col:28> '__wrap_iter<int *> &(*)() noexcept' <FunctionToPointerDecay>
  | | | `-DeclRefExpr 0x120dd5718 <col:28> '__wrap_iter<int *> &() noexcept' lvalue CXXMethod 0x120dd0528 'operator++' '__wrap_iter<int *> &() noexcept'
  | | `-DeclRefExpr 0x120dd56f8 <col:28> 'iterator':'std::__wrap_iter<int *>' lvalue Var 0x120dce268 '__begin1' 'iterator':'std::__wrap_iter<int *>'
  | |-DeclStmt 0x120dcdf38 <col:8, col:34>
  | | `-VarDecl 0x120dcded0 <col:8, col:28> col:20 used element 'int const &' cinit
  | |   `-ImplicitCastExpr 0x120dd5b98 <col:28> 'int const':'const int' lvalue <NoOp>
  | |     `-CXXOperatorCallExpr 0x120dd5a20 <col:28> 'int':'int' lvalue '*'
  | |       |-ImplicitCastExpr 0x120dd5a08 <col:28> 'reference (*)() const noexcept' <FunctionToPointerDecay>
  | |       | `-DeclRefExpr 0x120dd58f0 <col:28> 'reference () const noexcept' lvalue CXXMethod 0x120dd0070 'operator*' 'reference () const noexcept'
  | |       `-ImplicitCastExpr 0x120dd58d8 <col:28> 'const std::__wrap_iter<int *>' lvalue <NoOp>
  | |         `-DeclRefExpr 0x120dd58b8 <col:28> 'iterator':'std::__wrap_iter<int *>' lvalue Var 0x120dce268 '__begin1' 'iterator':'std::__wrap_iter<int *>'
  | `-CompoundStmt 0x120dd5cf0 <col:36, line:8:3>
  |   `-CompoundAssignOperator 0x120dd5cc0 <line:7:5, col:15> 'int' lvalue '+=' ComputeLHSTy='int' ComputeResultTy='int'
  |     |-DeclRefExpr 0x120dd5c68 <col:5> 'int' lvalue Var 0x120dcdda0 'result' 'int'
  |     `-ImplicitCastExpr 0x120dd5ca8 <col:15> 'int':'int' <LValueToRValue>
  |       `-DeclRefExpr 0x120dd5c88 <col:15> 'int const':'const int' lvalue Var 0x120dcded0 'element' 'int const &'
  `-ReturnStmt 0x120dd5d40 <line:9:3, col:10>
    `-ImplicitCastExpr 0x120dd5d28 <col:10> 'int' <LValueToRValue>
      `-DeclRefExpr 0x120dd5d08 <col:10> 'int' lvalue Var 0x120dcdda0 'result' 'int'
</pre>
</div>
</div>



<aside class="notes"><p>For example, how many AST nodes do you think this tiny function requires? Not
<code>std::vector</code> or anything from the header, <em>JUST</em> this function.</p>
<p>It&rsquo;s 8 lines long, probably more than 8 AST nodes.</p>
<p>22 words here, and some of those seem a bit redundant, like <code>{</code> and <code>}</code> are
separate.</p>
<p>So maybe 20?</p>
<p>Nope, 48 AST nodes. 48.</p>
<p>So, yeah, ASTs are not especially cache friendly.</p>
</aside>
</section><section>
<h1 class="r-fit-text" id="do-we-need-a-better-approach">Do we need a better approach?</h1>



<aside class="notes"><p>Ok, so this isn&rsquo;t really the most efficient approach. But it is <em>really</em> natural
to model everything this way.</p>
<p>So that somewhat raises the question &ndash; do we actually need a better model?</p>
</aside>
</section>
    <section><h1 id="carbons-compile-time-goals">Carbon&rsquo;s compile-time goals</h1>



<aside class="notes"></aside>
</section><section>
<h2 id="carbon-has-a-goal-of-fast-compile-times">Carbon has a goal of fast compile times:</h2>
<blockquote>
<p>Software development iteration has a critical &ldquo;edit, test, debug&rdquo; cycle.
Developers will use IDEs, editors, compilers, and other tools that need
different levels of parsing. For small projects, raw parsing speed is
essential; for large software systems, scalability of parsing is also
necessary.</p>
<p>&ndash; <a href="https://github.com/carbon-language/carbon-lang/blob/trunk/docs/project/goals.md#fast-and-scalable-development">Carbon&rsquo;s goal for fast and scalable development</a></p>
</blockquote>



<aside class="notes"></aside>
</section><section>
<h2 id="not-just-about-compiling-to-a-binary">Not just about compiling to a binary&hellip;</h2>
<ul>
<li>Need fast <em>tooling</em>:
<ul>
<li>Formatting</li>
<li>Jump-to-definition</li>
<li>Refactoring tools and other IDE plugins</li>
</ul>
</li>
<li>Many of these need <em>interactive</em> levels of speed
<ul>
<li><code>distcc</code> and other distributed compute approaches don&rsquo;t apply</li>
</ul>
</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="we-set-ourselves-a-challenge">We set ourselves a challenge:</h2>
<ul>
<li>Parse and lex at 

<span class='fragment ' ><p><strong>10 million lines of code /
second</strong></p>
</span>

</li>
<li>Semantic analysis at 

<span class='fragment ' ><p><strong>1 million lines of code /
second</strong></p>
</span>

</li>
<li>Lower to a binary at 

<span class='fragment ' ><p><strong>0.1 million lines of code /
second</strong></p>
</span>



<span class='fragment ' >Maybe&hellip;</span>

</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="thinking-about-it-the-other-way-is-eye-opening">Thinking about it the other way is eye opening:</h2>
<ul>
<li>Average budget of <strong><em>100 ns</em></strong> per line to lex and parse</li>
<li>Average budget of <strong><em>1 Œºs</em></strong> per line for semantic analysis</li>
<li>Let&rsquo;s think about these in terms of our latency numbers&hellip;</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="latency-numbers-table">Latency numbers table:</h2>
<table>
<thead>
<tr>
<th>Operation</th>
<th>Time in ns</th>
<th>Time in ms</th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU cycle</td>
<td>0.3 - 0.5</td>
<td></td>
</tr>
<tr>
<td>L1 cache reference</td>
<td>1</td>
<td></td>
</tr>
<tr>
<td>Branch misprediction</td>
<td>3</td>
<td></td>
</tr>
<tr>
<td>L2 cache reference</td>
<td>4</td>
<td></td>
</tr>
<tr>
<td>Mutex lock/unlock</td>
<td>17</td>
<td></td>
</tr>
<tr>
<td>Main memory reference</td>
<td>100</td>
<td></td>
</tr>
<tr>
<td>SSD Random Read</td>
<td>17,000</td>
<td>0.017</td>
</tr>
<tr>
<td>Read 1 MB sequentially from memory</td>
<td>38,000</td>
<td>0.038</td>
</tr>
<tr>
<td>Read 1 MB sequentially from SSD</td>
<td>622,000</td>
<td>0.622</td>
</tr>
</tbody>
</table>



<aside class="notes"></aside>
</section><section>
<h2 id="mapping-those-onto-our-budgets">Mapping those onto our budgets</h2>
<ul>
<li>About 200-300 cycles, maybe 500 instructions, per <strong><em>line</em></strong> lexed &amp; parsed</li>
<li>Only one main memory access per <strong><em>line</em></strong> lexed &amp; parsed (on average)!!!</li>
<li>Can&rsquo;t allocate memory per token&hellip; or line of tokens&hellip;</li>
<li>So many approaches just stop being viable if you want to hit this</li>
<li>Have to use every byte of every cache line accessed to have any hope</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h1 id="heading">üò±üò±üò±</h1>



<aside class="notes"><p>TODO: Maybe a goofy cat picture here instead of emojis?</p>
</aside>
</section><section>
<h2 id="ok-so-how-are-we-going-to-do-this">Ok, so how are we going to do this?</h2>



<aside class="notes"></aside>
</section>
    <section><h1 id="data-oriented-_compiler_-design">Data-oriented <em>compiler</em> design</h1>



<aside class="notes"></aside>
</section><section>
<h2 id="data-oriented-compiler-design">Data-oriented compiler design</h2>
<ul>
<li>Based on data-oriented design popularized in the games industry</li>
<li>Locality alone isn&rsquo;t a good model for modern hardware</li>
<li>All inputs likely in memory, no slow disk or memory in practice</li>
<li>Little or no &ldquo;interesting&rdquo; computation involved</li>
<li>Memory- and cache-bandwidth will almost always be limiting factor</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="simple-and-memory-dense-rather-than-lazy">Simple and memory-dense rather than lazy</h2>
<ul>
<li>Inputs all fit in memory, no underlying reason for laziness</li>
<li>Emphasis on processing input near memory bandwidth speed</li>
<li>Need resulting memory representation to be similarly dense as input</li>
<li>Even for (very) large inputs, no reason to add complexity to eager approach
<ul>
<li>10k lines of code (&lt; 40 bytes / line) easily fits into cache across wide
range of hardware</li>
<li>Streaming through cache at memory bandwidth seems easily acceptable speed</li>
<li>Can focus on efficient, tight processing</li>
</ul>
</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="core-data-structure-pattern">Core data structure pattern:</h2>
<ul>
<li>Primary homogenous dense array packed with most ubiquitous data</li>
<li>Indexed access to allow most connections to use <em>adjacency</em>
<ul>
<li>Avoids storage for these connections</li>
</ul>
</li>
<li>Side arrays for each different kind of secondary data needed
<ul>
<li>Connected to primary array with compressed index (smaller than pointer)</li>
</ul>
</li>
<li>Flyweight handles wrap indices and provide keys to the APIs</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="advantages-of-this-pattern">Advantages of this pattern:</h2>
<ul>
<li>Can densely pack main array, almost every bit is used
<ul>
<li>Differently shaped data factored into their own densely packed arrays</li>
<li>Enumerated states or bit-pack flags to fully use every byte of cache</li>
</ul>
</li>
<li>Indexed access to the &ldquo;next&rdquo; node doesn&rsquo;t depend on reading the current node
<ul>
<li>Processor can pipeline these memory accesses</li>
</ul>
</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="lets-look-at-how-this-manifests-at-every-layer">Let&rsquo;s look at how this manifests at every layer</h2>



<aside class="notes"><p>First up, the lexer</p>
</aside>
</section>
    <section><h2 id="data-oriented-lexing">Data-oriented lexing!</h2>



<aside class="notes"></aside>
</section><section>
<h2 id="lexing-directly-fits-the-desired-pattern">Lexing directly fits the desired pattern</h2>
<ul>
<li>Lex into a token buffer
<ul>
<li>Dense array of tokens</li>
<li>Side arrays of identifiers, strings, literals</li>
</ul>
</li>
<li>Expose a &ldquo;token&rdquo; as a flyweight index into the buffer</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="lexing-details-source-locations">Lexing details: source locations</h2>
<ul>
<li>Classical challenge is mapping tokens back to source locations
<ul>
<li>Very expensive &ndash; one of the most optimized parts of Clang&rsquo;s lexer</li>
</ul>
</li>
<li>Carbon instead has a token <em>be</em> the source location
<ul>
<li>Directly encodes its location within the source</li>
<li>Computes the location <em>extents</em> on demand</li>
</ul>
</li>
<li>Result is a single 32-bit token ID gives both token &amp; location</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="lexing-details-balanced-delimiters">Lexing details: balanced delimiters</h2>
<ul>
<li>Another challenge in parsing is recovering from unbalanced delimiters</li>
<li>To make the parser much simpler, lexer pre-computes balanced delimiters
<ul>
<li>Also lets it synthesize tokens to re-balance when necessary</li>
<li>Parser never has to handle unbalanced token streams</li>
</ul>
</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="lexing-implementation-a-guided-tour-live">Lexing implementation: a guided tour, live!</h2>



<aside class="notes"><p>Areas to cover:</p>
<ul>
<li>macro-generation of enums</li>
<li>macro-generation of tokens</li>
<li>the core tokenized buffer data structure
<ul>
<li>flyweight API</li>
<li>identifiers</li>
<li>literals</li>
</ul>
</li>
</ul>
</aside>
</section>
    <section><h2 id="data-oriented-parsing">Data-oriented parsing!</h2>



<aside class="notes"></aside>
</section><section>
<h2 id="challenge-how-to-represent-a-_tree_">Challenge: how to represent a <em>tree</em></h2>
<ul>
<li>Linearize the tree into an array based on expected iteration
<ul>
<li>Makes the common <em>traversal</em> a linear walk</li>
<li>Makes the hot edges between nodes be adjacency, breaking dependency chain</li>
<li>Postorder traversal ends up being the most useful</li>
</ul>
</li>
<li>Force 1:1 correspondence to tokens
<ul>
<li>Simpler allocation</li>
<li>A single edge to a token is always sufficient, no ranges, etc.</li>
</ul>
</li>
<li>Leverage introducers to ensure the traversal &ldquo;brackets&rdquo; constructs</li>
<li>Result ends up being essentially a stack machine for <em>observing</em> the parsed structure</li>
</ul>



<aside class="notes"></aside>
</section><section>
<pre><code class="language-carbon">var x: i32 = y + 1;
// TokenizedBuffer:
//     --------
// 1)  | var  |
//     --------
// 2)  | x    |
//     --------
// 3)  | :    |
//     --------
// 4)  | i32  |
//     --------
// 5)  | =    |
//     --------
// 6)  | y    |
//     --------
// 7)  | +    |
//     --------
// 8)  | 1    |
//     --------
// 9)  | ;    |
//     --------
</code></pre>



<aside class="notes"></aside>
</section><section>
<pre><code class="language-carbon">                     var    x    :    i32    =    y    +    1    ;
// TokenizedBuffer:
//     --------
// 1)  | var  |
//     --------
// 2)  | x    |
//     --------
// 3)  | :    |
//     --------
// 4)  | i32  |
//     --------
// 5)  | =    |
//     --------
// 6)  | y    |
//     --------
// 7)  | +    |                                 ( y )     ( 1 )
//     --------                                    \__   __/
// 8)  | 1    |                                      ( + )
//     --------
// 9)  | ;    |
//     --------
                     var    x    :    i32    =    y    +    1    ;
</code></pre>
</section><section>
<pre><code class="language-carbon">                     var    x    :    i32    =    y    +    1    ;
// TokenizedBuffer:
//     --------
// 1)  | var  |
//     --------
// 2)  | x    |
//     --------
// 3)  | :    |
//     --------
// 4)  | i32  |
//     --------
// 5)  | =    |
//     --------
// 6)  | y    |
//     --------
// 7)  | +    |           ( x )     ( i32 )     ( y )     ( 1 )
//     --------              \__   __/             \__   __/
// 8)  | 1    |                ( : )                 ( + )
//     --------
// 9)  | ;    |
//     --------
                     var    x    :    i32    =    y    +    1    ;
</code></pre>
</section><section>
<pre><code class="language-carbon">                     var    x    :    i32    =    y    +    1    ;
// TokenizedBuffer:
//     --------
// 1)  | var  |
//     --------
// 2)  | x    |
//     --------
// 3)  | :    |
//     --------
// 4)  | i32  |
//     --------
// 5)  | =    |
//     --------
// 6)  | y    |
//     --------
// 7)  | +    |           ( x )     ( i32 )     ( y )     ( 1 )
//     --------              \__   __/             \__   __/
// 8)  | 1    |    ( var )     ( : )       ( = )     ( + )
//     --------        \__________\___________\_________\______
// 9)  | ;    |                                                ( ; )
//     --------
                     var    x    :    i32    =    y    +    1    ;
</code></pre>
</section><section>
<pre><code class="language-carbon">                     var    x    :    i32    =    y    +    1    ;
// TokenizedBuffer:
//     --------
// 1)  | var  |
//     --------
// 2)  | x    |
//     --------
// 3)  | :    |
//     --------
// 4)  | i32  |
//     --------
// 5)  | =    |
//     --------
// 6)  | y    |
//     --------
// 7)  | +    |           ( `&lt;2&gt;x` )     ( `&lt;3&gt;i32` )     ( `&lt;6&gt;y` )     ( `&lt;7&gt;1` )
//     --------              \__   __/             \__   __/
// 8)  | 1    |    ( `&lt;1&gt;var` )     ( `&lt;4&gt;:` )       ( `&lt;5&gt;=` )     ( `&lt;8&gt;+` )
//     --------        \__________\___________\_________\______
// 9)  | ;    |                                                ( `&lt;9&gt;;` )
//     --------
                     var    x    :    i32    =    y    +    1    ;
</code></pre>
</section><section>
<pre><code class="language-carbon">                     var    x    :    i32    =    y    +    1    ;
// TokenizedBuffer:
//     --------
// 1)  | var  |    ( var )
//     --------       |
// 2)  | x    |       |   ( x )
//     --------       |     |
// 3)  | :    |       |     |       ( i32 )
//     --------       |      \__   __/
// 4)  | i32  |       |        ( : )
//     --------       |          |
// 5)  | =    |       |          |         ( = )
//     --------       |          |           |
// 6)  | y    |       |          |           |  ( y )
//     --------       |          |           |    |
// 7)  | +    |       |          |           |    |       ( 1 )
//     --------       |          |           |     \__   __/
// 8)  | 1    |       |          |           |       ( + )
//     --------        \__________\___________\_________\______
// 9)  | ;    |                                                ( ; )
//     --------
                     var    x    :    i32    =    y    +    1    ;
</code></pre>
</section><section>
<pre><code class="language-carbon">                     var    x    :    i32    =    y    +    1    ;
// TokenizedBuffer:                                                   ParseTree:
//     --------                                                        --------
// 1)  | var  |    ( `&lt;1&gt;var` )                                             | `&lt;1&gt;var`  |
//     --------       |                                                --------
// 2)  | x    |       |   ( `&lt;2&gt;x` )                                        | `&lt;2&gt;x`    |
//     --------       |     |                                          --------
// 3)  | :    |       |     |       ( `&lt;3&gt;i32` )                            | `&lt;3&gt;i32`  |
//     --------       |      \__   __/                                 --------
// 4)  | i32  |       |        ( `&lt;4&gt;:` )                                   | `&lt;4&gt;:`    |
//     --------       |          |                                     --------
// 5)  | =    |       |          |         ( `&lt;5&gt;=` )                       | `&lt;5&gt;=`    |
//     --------       |          |           |                         --------
// 6)  | y    |       |          |           |  ( `&lt;6&gt;y` )                  | `&lt;6&gt;y`    |
//     --------       |          |           |    |                    --------
// 7)  | +    |       |          |           |    |       ( `&lt;7&gt;1` )        | `&lt;7&gt;1`    |
//     --------       |          |           |     \__   __/           --------
// 8)  | 1    |       |          |           |       ( `&lt;8&gt;+` )             | `&lt;8&gt;+`    |
//     --------        \__________\___________\_________\______        --------
// 9)  | ;    |                                                ( `&lt;9&gt;;` )   | `&lt;9&gt;;`    |
//     --------                                                        --------
                     var    x    :    i32    =    y    +    1    ;
</code></pre>



<aside class="notes"></aside>
</section><section>
<h2 id="parse-tree-implementation-a-guided-tour-live">Parse tree implementation: a guided tour, live!</h2>



<aside class="notes"></aside>
</section><section>
<h2 id="how-does-the-parser-build-the-tree">How does the parser build the tree?</h2>
<ul>
<li>Technically a recursive descent parser</li>
<li>Faces a classic problem &ndash; deep recursion exhausting the call stack
<ul>
<li>C++ and Clang have this problem as well</li>
<li>Has led to a number of &ldquo;exciting&rdquo; tricks to work around it</li>
<li>Especially problematic for library usage of the compiler</li>
</ul>
</li>
<li>Carbon creates a dedicated stack data structure for its parser
<ul>
<li>Turns the parse into a &ldquo;normal&rdquo; state machine without recursive calls</li>
</ul>
</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="parser-implementation-more-live-tour">Parser implementation: more live tour!</h2>



<aside class="notes"></aside>
</section>
    <section><h2 id="data-oriented-semantics">Data-oriented semantics!</h2>



<aside class="notes"></aside>
</section><section>
<h2 id="core-idea-model-semantics-_as-an-ir_">Core idea: model semantics <em>as an IR</em></h2>
<ul>
<li>IR here in the sense of a compiler&rsquo;s Intermediate Representation
<ul>
<li>Generally structured in blocks with an order of execution</li>
</ul>
</li>
<li>For runtime code, this generates exactly the structure we want
<ul>
<li>First, to type check and validate the semantics</li>
<li>Then, to lower into a lower level IR like LLVM&rsquo;s</li>
</ul>
</li>
<li>We map other parts of semantic analysis into the IR space:
<ul>
<li>Declaring names, types, etc., are compile time evaluation</li>
<li>Type checking is evaluation, constant propagation, and then verifying</li>
<li>Template instantiation similar to &ldquo;specialization&rdquo; in optimizing compilers</li>
</ul>
</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="result-feels-a-_bit_-like-a-metaprogramming-ir">Result feels a <em>bit</em> like a &ldquo;metaprogramming&rdquo; IR</h2>
<ul>
<li>Imperative model for building all constructs in the language</li>
<li>Lends itself to well understood techniques for fast compile time:
<ul>
<li>Interpreters</li>
<li>Efficient streaming data structures</li>
</ul>
</li>
<li>Some hope that this lends itself to implementing actual metaprogramming features
<ul>
<li>But <em>way</em> too early to tell&hellip;</li>
</ul>
</li>
<li>Really, this is <em>very</em> early and an area of active work</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="live-_tiniest_-demo-of-semantics-ir">Live <em>tiniest</em> demo of Semantics IR</h2>



<aside class="notes"></aside>
</section><section>
<h2 id="built-by-walking-the-postorder-parse-tree">Built by walking the postorder parse tree</h2>
<ul>
<li>Primary consumer driving the parse tree design</li>
<li>Walk observes the parse structure
<ul>
<li>Uses a stack data structure to provide any context or other details</li>
</ul>
</li>
<li>Efficiency hinges on a single pass over the parse tree
<ul>
<li>This is difficult though</li>
<li>Some places we defer subtrees: nested function bodies</li>
<li>Other places use intermediate data structures</li>
<li>Template instantiations are a fundamental challenge, but tolerable</li>
</ul>
</li>
</ul>
</section><section>
<h2 id="gets-more-help-from-the-language">Gets more help from the language</h2>
<ul>
<li>Single pass is easier if we emit things before they&rsquo;re needed</li>
<li>Carbon has a principle of <a href="https://github.com/carbon-language/carbon-lang/blob/trunk/docs/project/principles/information_accumulation.md">information accumulation</a>
<ul>
<li>Toolchain leverages this to directly emit complete semantics</li>
<li>Limiting <em>necessary</em> repeated passes to monomorphization &amp; instantiation</li>
</ul>
</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="again-really-early-days-on-semanticsbrmore-to-come">Again, really early days on semantics.<br/>More to come!</h2>



<aside class="notes"></aside>
</section>
    <section><h1 id="data-oriented-lowering">Data-oriented lowering!</h1>



<aside class="notes"></aside>
</section><section>
<h2 id="eh-not-really">Eh&hellip; not really&hellip;</h2>



<aside class="notes"></aside>
</section><section>
<h2 id="somewhat-data-oriented-lowering">Somewhat data-oriented lowering?</h2>
<ul>
<li>Because of the semantics IR, some aspects of lowering will fall out:
<ul>
<li>Traversing the IR to emit LLVM IR will be fast and efficient</li>
<li>Other operations like high-level optimizations or constant evaluation similarly</li>
</ul>
</li>
<li>But LLVM itself is <em>not</em> especially data-oriented in its design</li>
<li>Currently no plan to try to change this, but interesting area of future work</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="ultimately-limited-by-llvm-today">Ultimately, limited by LLVM today</h2>
<ul>
<li>LLVM heavily uses sparse, pointer-based data structures</li>
<li>Tends to be very inefficient, often &gt;50% cycles stalled on data
<ul>
<li>Despite heavily optimized data structures below the core IR</li>
</ul>
</li>
</ul>
<div class="fragment">
<p>Also, we have a long way to go before this can be our focus!</p>
</div>
</section>
    <section><h2 id="aside-testing-the-compiler">Aside: testing the compiler</h2>



<aside class="notes"></aside>
</section><section>
<h2 id="basic-testing-follows-usual-patterns">Basic testing follows usual patterns</h2>
<ul>
<li>Easier to write focused unit tests due to simpler language
<ul>
<li>No preprocessor in the lexer, etc.</li>
</ul>
</li>
<li>Serializing each layer into line delimited JSON
<ul>
<li>Easy to do quick things with <code>grep</code></li>
<li>Can use powerful tools like <code>jq</code></li>
</ul>
</li>
<li>LLVM <a href="https://llvm.org/docs/CommandGuide/FileCheck.html">FileCheck</a>-style testing throughout</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="fuzz-testing-is-a-more-interesting-challenge">Fuzz testing is a more interesting challenge</h2>
<ul>
<li>Historically, C++ compilers have been ~impossible to fuzz test
<ul>
<li>None were built with continuous fuzz testing</li>
<li>Very large number of failures throughout the system</li>
<li>Difficult to create interesting inputs</li>
</ul>
</li>
<li>Carbon decided to have fuzz testing from day one
<ul>
<li>And to fuzz test each layer</li>
</ul>
</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="fuzz-testing-with-more-complex-inputs">Fuzz testing with more complex inputs</h2>
<ul>
<li>Also developed a protobuf-based fuzzer</li>
<li>Models the Carbon AST in protobuf messages</li>
<li>Uses structural fuzz testing systems build on top of protobufs to synthesize interesting and complex ASTs</li>
<li>Renders them into source as fuzzer inputs</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="our-goal-a-compiler-that-_does-not-crash_">Our goal: a compiler that <em>does not crash</em></h2>
<ul>
<li>May still have plenty of bugs, but they shouldn&rsquo;t crash</li>
<li>Users should always get a useful error message</li>
<li>Only way we know to achieve this is to do it from the very beginning</li>
</ul>



<aside class="notes"></aside>
</section>
    <section><h1 id="some-key-takeaways">Some key takeaways&hellip;</h1>
</section><section>
<h2 id="re-think-traditional-compiler-design">Re-think traditional compiler design</h2>
</section><section>
<h2 id="challenge-assumptions-with-aggressive-goals">Challenge assumptions with aggressive goals</h2>
</section><section>
<h2 id="compiler--implementation-design-should-be-a-major-input-to-language-design">Compiler &amp; implementation design should be a major input to language design</h2>
</section><section>
<h2 id="whats-next-for-carbons-toolchain">What&rsquo;s next for Carbon&rsquo;s toolchain?</h2>



<aside class="notes"></aside>
</section><section>
<h2 id="future-carbon-toolchain-efforts-include">Future Carbon toolchain efforts include</h2>
<ul>
<li>Complete semantic analysis and lowering</li>
<li>Interop with C++ through Clang (come see tomorrow&rsquo;s keynote!)</li>
<li>Bundling and being able to access a complete Clang-based C++ toolchain</li>
<li>Modern error messages and diagnostics</li>
<li>Building a <code>carbon-format</code> tool on top of the parser</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="future-carbon-toolchain-efforts-include-1">Future Carbon toolchain efforts include</h2>
<ul>
<li>Generating arbitrary amounts of random Carbon source code
<ul>
<li>Parameterized distribution of language constructs</li>
<li>Parameterized distribution of name &amp; comment lengths</li>
<li>Able to replicate representative distributions or edge cases</li>
</ul>
</li>
<li>Aggressive benchmarking &amp; optimization of each layer
<ul>
<li>Maybe even generating C++ code for comparative benchmarking</li>
</ul>
</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h2 id="all-part-of-carbons-ongoing-efforts-to">All part of Carbon&rsquo;s ongoing efforts to</h2>
<ul>
<li>Modernize compiler design and implementation</li>
<li>And deliver <em>radical</em> improvements to compile times</li>
</ul>



<aside class="notes"></aside>
</section><section>
<h1 id="thank-you">Thank you!</h1>



<aside class="notes"></aside>
</section><section>
<h2 id="resources-and-more-information">Resources and more information:</h2>
<ul>
<li><a href="https://github.com/carbon-language/carbon-lang#getting-started">https://github.com/carbon-language/carbon-lang#getting-started</a></li>
<li><a href="https://github.com/carbon-language/carbon-lang/tree/trunk/toolchain">https://github.com/carbon-language/carbon-lang/tree/trunk/toolchain</a>
<ul>
<li><a href="https://docs.google.com/document/d/1RRYMm42osyqhI2LyjrjockYCutQ5dOf8Abu50kTrkX0/edit?resourcekey=0-kHyqOESbOHmzZphUbtLrTw">Design doc</a> linked here and from the toolchain tree above</li>
</ul>
</li>
<li><a href="https://discord.gg/znvPNhd7">Toolchain channel</a> of our <a href="https://discord.gg/NECBAaZ4">Discord</a> server</li>
</ul>



<aside class="notes"></aside>
</section>

</div>
      
    </div>
<script type="text/javascript" src=../../cc/reveal-hugo/object-assign.js></script>


<script src="../../cc/reveal-js/dist/reveal.js"></script>


  <script type="text/javascript" src="../../cc/reveal-js/plugin/markdown/markdown.js"></script>
  
  <script type="text/javascript" src="../../cc/reveal-js/plugin/highlight/highlight.js"></script>
  
  <script type="text/javascript" src="../../cc/reveal-js/plugin/zoom/zoom.js"></script>
  
  <script type="text/javascript" src="../../cc/reveal-js/plugin/notes/notes.js"></script>
  
<script type="text/javascript">
  
  
  function camelize(map) {
    if (map) {
      Object.keys(map).forEach(function(k) {
        newK = k.replace(/(\_\w)/g, function(m) { return m[1].toUpperCase() });
        if (newK != k) {
          map[newK] = map[k];
          delete map[k];
        }
      });
    }
    return map;
  }

  var revealHugoDefaults = { center: true, controls: true, history: true, progress: true, transition: "slide" };
  var revealHugoSiteParams = {"controls":false,"custom_theme":"css/reveal/custom-theme.scss","custom_theme_compile":true,"custom_theme_options":{"enablesourcemap":true,"targetpath":"css/reveal/custom-theme.css"},"hash":true,"hash_one_based_index":true,"height":900,"load_default_plugins":false,"margin":0.08,"navigation_mode":"linear","progress":false,"raw_initialize_options":"highlight: { beforeHighlight: beforeHighlightHook, },","slide_number":"c","transition":"none","width":1600};
  var revealHugoPageParams = {};

  var revealHugoPlugins = {
    
    plugins: [RevealMarkdown,RevealHighlight,RevealZoom,RevealNotes]
  };

  
  var options = Object.assign(
    {
      highlight: { beforeHighlight: beforeHighlightHook, },
    },
    camelize(revealHugoDefaults),
    camelize(revealHugoSiteParams),
    camelize(revealHugoPageParams),
    camelize(revealHugoPlugins));

  Reveal.initialize(options);
</script>





  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  





    <script type="text/javascript">
Reveal.on('slidechanged', function(event) {
  if (event.currentSlide.classList.contains('auto-advance')) {
    Reveal.nextFragment();
  }
});
Reveal.on('fragmenthidden', function(event) {
  if (event.fragment.attributes['data-fragment-index'].value == "0") {
    slide = document.querySelector("section.present.auto-advance")
    if (slide) {
      Reveal.prev();
    }
  }
});
</script>

    
  </body>
</html>
